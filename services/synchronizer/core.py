"""
–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞–º–∏
"""

import pandas as pd
from openpyxl import load_workbook
from openpyxl.utils import range_boundaries
from typing import Dict, Tuple, Optional, List
from pathlib import Path
import sys

from config.config import FILE_CONFIGS, is_excluded_column
from utils.logger_config import setup_logger
from .constants import ARTICLE_COLUMNS, DIMENSIONS_MAPPING
from .converters import ValueConverter
from .dimensions import DimensionsSynchronizer
from .alignment import ArticleAligner
from .validation import ValidationChain

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logger = setup_logger('data_sync')


class DataSynchronizer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É —Ç—Ä–µ–º—è –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞–º–∏"""
    
    def __init__(self, comparison_result: Dict, ai_comparator=None):
        """
        Args:
            comparison_result: —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ AIComparator
            ai_comparator: —ç–∫–∑–µ–º–ø–ª—è—Ä AIComparator –¥–ª—è AI-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        self.comparison_result = comparison_result
        self.article_columns = ARTICLE_COLUMNS
        self.changes_log = {
            'wildberries': [],
            'ozon': [],
            'yandex': []
        }
        self.original_file_paths = {}
        self.ai_comparator = ai_comparator
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        self.validation_chain = ValidationChain(ai_comparator)
        self.column_validations = {}  # {marketplace: {column_name: [allowed_values]}}
        self.original_column_names = {}
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataSynchronizer")
        logger.debug(f"AI comparator –ø–µ—Ä–µ–¥–∞–Ω: {ai_comparator is not None}")
    
    @property
    def ai_validation_log(self):
        """Getter –¥–ª—è –ª–æ–≥–æ–≤ AI-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        return self.validation_chain.ai_validation_log
    
    def synchronize_data(
        self,
        file_paths: Dict[str, str],
        output_paths: Dict[str, str] = None,
        report_path: str = None
    ) -> Tuple[Dict[str, pd.DataFrame], Dict]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            file_paths: –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤
            output_paths: –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            report_path: –ø—É—Ç—å –∫ –æ—Ç—á—ë—Ç—É
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ DataFrame, –ª–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        """
        logger.info("="*60)
        logger.info("–°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• –ú–ï–ñ–î–£ –ú–ê–†–ö–ï–¢–ü–õ–ï–ô–°–ê–ú–ò")
        logger.info("="*60)
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ç—Ä–µ—Ö —Ñ–∞–π–ª–æ–≤
        dfs = self._load_all_dataframes(file_paths)
        
        # 2. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∞—Ä—Ç–∏–∫—É–ª—ã
        logger.info("\n[*] –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∞—Ä—Ç–∏–∫—É–ª–æ–≤...")
        dfs = ArticleAligner.align_articles(dfs)
        
        # 3. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã—Ö –≥–∞–±–∞—Ä–∏—Ç–æ–≤
        logger.info("\n[*] –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥–∞–±–∞—Ä–∏—Ç–æ–≤...")
        dimensions_synced = DimensionsSynchronizer.sync_dimensions(dfs)
        
        # 4. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—Ö–µ–º–µ
        synced_dfs = self._sync_all_matches(dfs)
        
        # 5. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–∞–±–∞—Ä–∏—Ç–æ–≤ WB (–º–º ‚Üí —Å–º –µ—Å–ª–∏ –∏–∑ Ozon)
        logger.info("\n[*] –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–∞–±–∞—Ä–∏—Ç–æ–≤ WB...")
        converted_count = self._postprocess_wb_dimensions(synced_dfs)
        if converted_count > 0:
            logger.info(f"‚úÖ –°–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {converted_count} –∑–Ω–∞—á–µ–Ω–∏–π –≥–∞–±–∞—Ä–∏—Ç–æ–≤ (–º–º ‚Üí —Å–º)")
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if output_paths:
            self._save_results(synced_dfs, output_paths)
        
        logger.info("‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        return synced_dfs, self.changes_log
    
    def _load_all_dataframes(self, file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ openpyxl –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        logger.info("üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤...")
        dfs = {}
        self.original_column_names = {}
        
        for marketplace, file_path in file_paths.items():
            self.original_file_paths[marketplace] = file_path
            config = FILE_CONFIGS[marketplace]
            
            wb = load_workbook(file_path, data_only=True)
            ws = wb[config['sheet_name']]
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º validation –ø—Ä–∞–≤–∏–ª–∞
            self._load_column_validations(ws, marketplace, config)
            
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = []
            headers = []
            
            # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            for cell in ws[config['header_row']]:
                headers.append(cell.value if cell.value else '')
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç–æ–ª–±—Ü–æ–≤
            headers = self._handle_duplicate_columns(headers, marketplace)
            
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data_start = config.get('data_start_row', config['header_row'] + 1)
            for row in ws.iter_rows(min_row=data_start, values_only=True):
                data.append(row)
            
            df = pd.DataFrame(data, columns=headers)
            dfs[marketplace] = df
            wb.close()
            
            logger.info(f"‚úÖ {config['display_name']}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ç–æ–≤–∞—Ä–æ–≤")
        
        return dfs
    
    def _handle_duplicate_columns(self, headers: List[str], marketplace: str) -> List[str]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç–æ–ª–±—Ü–æ–≤, –¥–æ–±–∞–≤–ª—è—è —Å—É—Ñ—Ñ–∏–∫—Å—ã"""
        original_headers = headers.copy()
        seen = {}
        renamed_columns = {}
        
        for i, col in enumerate(headers):
            if col in seen:
                # –ù–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç - –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å
                seen[col] += 1
                new_name = f"{col}{seen[col]}"
                logger.warning(f"‚ö†Ô∏è [{marketplace}] –î—É–±–ª–∏–∫–∞—Ç —Å—Ç–æ–ª–±—Ü–∞ '{col}' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ '{new_name}'")
                headers[i] = new_name
                renamed_columns[new_name] = col
            else:
                seen[col] = 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        if renamed_columns:
            self.original_column_names[marketplace] = {
                'renamed': renamed_columns,
                'all_headers': original_headers
            }
        
        return headers
    
    def _load_column_validations(self, ws, marketplace: str, config: Dict):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ validation –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞"""
        if marketplace not in self.column_validations:
            self.column_validations[marketplace] = {}
        
        header_row = config['header_row']
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥: –Ω–æ–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ -> –Ω–∞–∑–≤–∞–Ω–∏–µ
        col_idx_to_name = {}
        for col_idx, cell in enumerate(ws[header_row], start=1):
            if cell.value:
                col_name = str(cell.value).strip()
                col_idx_to_name[col_idx] = col_name
        
        logger.info(f"üìã [{marketplace}] –ù–∞–π–¥–µ–Ω–æ {len(col_idx_to_name)} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        workbook = ws.parent
        named_ranges = {}
        try:
            for name_obj in workbook.defined_names.values():
                try:
                    if name_obj.value:
                        named_ranges[name_obj.name] = name_obj.value
                except Exception as e:
                    logger.debug(f"[{marketplace}] –ü—Ä–æ–ø—É—â–µ–Ω –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: {e}")
            
            logger.info(f"[{marketplace}] –ù–∞–π–¥–µ–Ω–æ {len(named_ranges)} –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤")
        except Exception as e:
            logger.error(f"[{marketplace}] –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤: {e}")
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º validation –ø—Ä–∞–≤–∏–ª–∞–º
        validation_count = 0
        for dv_index, dv in enumerate(ws.data_validations.dataValidation, start=1):
            if dv.type != "list" or dv.sqref is None:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ validation
            allowed_values = self._extract_validation_values(dv, ws, workbook, named_ranges, marketplace, dv_index)
            
            if not allowed_values:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã —ç—Ç–∏–º validation
            ranges = str(dv.sqref).split()
            for range_str in ranges:
                try:
                    if ':' in range_str:
                        min_col, min_row, max_col, max_row = range_boundaries(range_str)
                        
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º validation –∫–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                        for col_idx in range(min_col, max_col + 1):
                            if col_idx in col_idx_to_name:
                                col_name = col_idx_to_name[col_idx]
                                self.column_validations[marketplace][col_name] = allowed_values
                                validation_count += 1
                                logger.info(f"‚úÖ [{marketplace}] Validation –¥–ª—è '{col_name}': {len(allowed_values)} –∑–Ω–∞—á–µ–Ω–∏–π")
                except Exception as e:
                    logger.error(f"[{marketplace}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ range_str '{range_str}': {e}")
        
        logger.info(f"üìä [{marketplace}] –ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ validation –¥–ª—è {validation_count} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    def _extract_validation_values(
        self, 
        dv, 
        ws, 
        workbook, 
        named_ranges: Dict, 
        marketplace: str, 
        dv_index: int
    ) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ø—Ä–∞–≤–∏–ª–∞ validation"""
        allowed_values = []
        
        if not dv.formula1:
            return allowed_values
        
        formula = dv.formula1
        
        # –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞–Ω –ø—Ä—è–º–æ: "–ö—Ä–∞—Å–Ω—ã–π,–°–∏–Ω–∏–π,–ó–µ–ª–µ–Ω—ã–π"
        if formula.startswith('"') and formula.endswith('"'):
            allowed_values = [v.strip() for v in formula.strip('"').split(',')]
            logger.debug(f"[{marketplace}] DV #{dv_index}: –ü—Ä—è–º–æ–π —Å–ø–∏—Å–æ–∫, {len(allowed_values)} –∑–Ω–∞—á–µ–Ω–∏–π")
        
        # –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        elif formula in named_ranges:
            try:
                range_formula = named_ranges[formula]
                clean_formula = range_formula.replace('$', '')
                
                if '!' in clean_formula:
                    sheet_name, range_ref = clean_formula.split('!', 1)
                    sheet_name = sheet_name.strip("'")
                    target_ws = workbook[sheet_name]
                else:
                    range_ref = clean_formula
                    target_ws = ws
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                for row in target_ws[range_ref]:
                    for cell in row:
                        if cell.value is not None:
                            allowed_values.append(str(cell.value).strip())
                
                logger.info(f"‚úÖ [{marketplace}] DV #{dv_index}: –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(allowed_values)} –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ '{formula}'")
            except Exception as e:
                logger.error(f"[{marketplace}] DV #{dv_index}: –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ '{formula}': {e}")
        
        # –û–±—ã—á–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        elif ':' in formula:
            try:
                clean_formula = formula.replace('$', '')
                if '!' in clean_formula:
                    sheet_name, range_ref = clean_formula.split('!')
                    target_ws = workbook[sheet_name]
                else:
                    range_ref = clean_formula
                    target_ws = ws
                
                for row in target_ws[range_ref]:
                    for cell in row:
                        if cell.value is not None:
                            allowed_values.append(str(cell.value).strip())
            except Exception as e:
                logger.error(f"[{marketplace}] DV #{dv_index}: –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è validation: {e}")
        
        return allowed_values
    
    def _sync_all_matches(self, dfs: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã"""
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã
        synced_dfs = {
            'wildberries': dfs['wildberries'].copy(),
            'ozon': dfs['ozon'].copy(),
            'yandex': dfs['yandex'].copy()
        }
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤—Å–µ—Ö —Ç—Ä–µ—Ö –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤
        print("\n[*] –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤—Å–µ—Ö 3 –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤...")
        synced_dfs = self._sync_three_way_matches(synced_dfs)
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞–º–∏
        print("\n[*] –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤...")
        synced_dfs = self._sync_two_way_matches(synced_dfs)
        
        return synced_dfs
    
    def _sync_three_way_matches(self, dfs: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤—Å–µ—Ö —Ç—Ä–µ—Ö –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤"""
        matches = self.comparison_result.get('matches_all_three', [])
        
        if not matches:
            print(" –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
            return dfs
        
        total_filled = 0
        skipped_count = 0
        
        for match in matches:
            col_wb = match.get('column_1')
            col_ozon = match.get('column_2')
            col_yandex = match.get('column_3')
            
            if not all([col_wb, col_ozon, col_yandex]):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            if (is_excluded_column(col_wb) or
                is_excluded_column(col_ozon) or
                is_excluded_column(col_yandex)):
                skipped_count += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç–æ–ª–±—Ü—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            if (col_wb not in dfs['wildberries'].columns or
                col_ozon not in dfs['ozon'].columns or
                col_yandex not in dfs['yandex'].columns):
                continue
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É —Ç—Ä–µ–º—è —Ñ–∞–π–ª–∞–º–∏
            filled = self._sync_three_columns(dfs, col_wb, col_ozon, col_yandex)
            
            if filled > 0:
                confidence = int(match.get('confidence', 0) * 100)
                print(f" ‚úì –ó–∞–ø–æ–ª–Ω–µ–Ω–æ {filled} –∑–Ω–∞—á–µ–Ω–∏–π: '{col_wb}' ‚Üî '{col_ozon}' ‚Üî '{col_yandex}' ({confidence}%)")
                total_filled += filled
        
        if skipped_count > 0:
            print(f"[!] –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped_count} –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        print(f"[+] –í—Å–µ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ {total_filled} –ø—É—Å—Ç—ã—Ö —è—á–µ–µ–∫ –≤ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è—Ö –≤—Å–µ—Ö 3 –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤")
        return dfs
    
    def _sync_two_way_matches(self, dfs: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤"""
        pairs = [
            ('matches_1_2', 'wildberries', 'ozon', 'column_1', 'column_2'),
            ('matches_1_3', 'wildberries', 'yandex', 'column_1', 'column_3'),
            ('matches_2_3', 'ozon', 'yandex', 'column_2', 'column_3')
        ]
        
        total_filled = 0
        skipped_count = 0
        
        for match_key, mp1, mp2, col_key1, col_key2 in pairs:
            matches = self.comparison_result.get(match_key, [])
            
            if not matches:
                continue
            
            for match in matches:
                col1 = match.get(col_key1)
                col2 = match.get(col_key2)
                
                if not all([col1, col2]):
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
                if is_excluded_column(col1) or is_excluded_column(col2):
                    skipped_count += 1
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç–æ–ª–±—Ü—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
                if col1 not in dfs[mp1].columns or col2 not in dfs[mp2].columns:
                    continue
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –¥–≤—É–º—è —Ñ–∞–π–ª–∞–º–∏
                filled = self._sync_two_columns(dfs, mp1, mp2, col1, col2)
                
                if filled > 0:
                    confidence = int(match.get('confidence', 0) * 100)
                    print(f" ‚úì –ó–∞–ø–æ–ª–Ω–µ–Ω–æ {filled} –∑–Ω–∞—á–µ–Ω–∏–π: {mp1}:'{col1}' ‚Üî {mp2}:'{col2}' ({confidence}%)")
                    total_filled += filled
        
        if skipped_count > 0:
            print(f"[!] –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped_count} –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        print(f"[+] –í—Å–µ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ {total_filled} –ø—É—Å—Ç—ã—Ö —è—á–µ–µ–∫ –≤ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è—Ö –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏")
        return dfs
    
    def _sync_three_columns(
        self,
        dfs: Dict[str, pd.DataFrame],
        col_wb: str,
        col_ozon: str,
        col_yandex: str
    ) -> int:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É —Ç—Ä–µ–º—è —Å—Ç–æ–ª–±—Ü–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ç–∏–∫—É–ª–æ–≤
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —è—á–µ–µ–∫
        """
        filled_count = 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
        unit_wb = ValueConverter.detect_unit(col_wb)
        unit_ozon = ValueConverter.detect_unit(col_ozon)
        unit_yandex = ValueConverter.detect_unit(col_yandex)
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É
        wb_data = self._create_article_map(dfs['wildberries'], ARTICLE_COLUMNS['wildberries'], col_wb)
        ozon_data = self._create_article_map(dfs['ozon'], ARTICLE_COLUMNS['ozon'], col_ozon)
        yandex_data = self._create_article_map(dfs['yandex'], ARTICLE_COLUMNS['yandex'], col_yandex)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã
        all_articles = set(wb_data.keys()) | set(ozon_data.keys()) | set(yandex_data.keys())
        
        for article in all_articles:
            if not article:
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            values = {
                'wildberries': wb_data.get(article, {}).get('value'),
                'ozon': ozon_data.get(article, {}).get('value'),
                'yandex': yandex_data.get(article, {}).get('value')
            }
            
            # –ù–∞—Ö–æ–¥–∏–º –Ω–µ–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏ –µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫
            source_value, source_unit = self._find_source_value(values, unit_wb, unit_ozon, unit_yandex)
            
            if source_value is None:
                continue
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–∞–∂–¥–æ–º –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–µ
            filled_count += self._fill_marketplace_value(
                dfs['wildberries'], article, col_wb, source_value, source_unit, unit_wb, 
                'wildberries', wb_data, values['wildberries']
            )
            
            filled_count += self._fill_marketplace_value(
                dfs['ozon'], article, col_ozon, source_value, source_unit, unit_ozon,
                'ozon', ozon_data, values['ozon']
            )
            
            # –î–ª—è –Ø–Ω–¥–µ–∫—Å–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –≥–∞–±–∞—Ä–∏—Ç—ã
            if col_yandex == DIMENSIONS_MAPPING['yandex']['composite']:
                filled = self._fill_composite_dimensions(
                    dfs, article, col_yandex, source_unit, unit_wb, unit_ozon, yandex_data, values['yandex']
                )
                filled_count += filled
            else:
                filled_count += self._fill_marketplace_value(
                    dfs['yandex'], article, col_yandex, source_value, source_unit, unit_yandex,
                    'yandex', yandex_data, values['yandex']
                )
        
        return filled_count
    
    def _sync_two_columns(
        self,
        dfs: Dict[str, pd.DataFrame],
        mp1: str,
        mp2: str,
        col1: str,
        col2: str
    ) -> int:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç–æ–ª–±—Ü–∞–º–∏"""
        filled_count = 0
        
        unit1 = ValueConverter.detect_unit(col1)
        unit2 = ValueConverter.detect_unit(col2)
        
        data1 = self._create_article_map(dfs[mp1], ARTICLE_COLUMNS[mp1], col1)
        data2 = self._create_article_map(dfs[mp2], ARTICLE_COLUMNS[mp2], col2)
        
        all_articles = set(data1.keys()) | set(data2.keys())
        
        for article in all_articles:
            if not article:
                continue
            
            val1 = data1.get(article, {}).get('value')
            val2 = data2.get(article, {}).get('value')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∫–∞–ª—è—Ä –µ—Å–ª–∏ Series
            if isinstance(val1, pd.Series):
                val1 = val1.iloc[0] if not val1.empty else None
            if isinstance(val2, pd.Series):
                val2 = val2.iloc[0] if not val2.empty else None
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º mp1 –∏–∑ mp2
            if (pd.isna(val1) or not str(val1).strip()) and pd.notna(val2) and str(val2).strip():
                filled_count += self._fill_marketplace_value(
                    dfs[mp1], article, col1, val2, unit2, unit1, mp1, data1, val1
                )
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º mp2 –∏–∑ mp1
            if (pd.isna(val2) or not str(val2).strip()) and pd.notna(val1) and str(val1).strip():
                filled_count += self._fill_marketplace_value(
                    dfs[mp2], article, col2, val1, unit1, unit2, mp2, data2, val2
                )
        
        return filled_count
    
    def _find_source_value(self, values: Dict, unit_wb, unit_ozon, unit_yandex):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–µ–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏ –µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫"""
        for marketplace, val in values.items():
            if isinstance(val, pd.Series):
                val = val.iloc[0] if not val.empty else None
            
            if pd.notna(val) and str(val).strip():
                if marketplace == 'wildberries':
                    return val, unit_wb
                elif marketplace == 'ozon':
                    return val, unit_ozon
                else:
                    return val, unit_yandex
        
        return None, None
    
    def _fill_marketplace_value(
        self, df: pd.DataFrame, article: str, col: str,
        source_value, source_unit, target_unit, marketplace: str,
        data_map: Dict, current_value
    ) -> int:
        """–ó–∞–ø–æ–ª–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–µ —Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        if isinstance(current_value, pd.Series):
            current_value = current_value.iloc[0] if not current_value.empty else None
        
        if pd.notna(current_value) and str(current_value).strip():
            return 0  # –£–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ
        
        if article not in data_map:
            return 0
        
        idx = data_map[article]['index']
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ
        converted_value = ValueConverter.convert_value(source_value, source_unit, target_unit)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        allowed_values = self.column_validations.get(marketplace, {}).get(col)
        final_value = self.validation_chain.validate_multiple_values(
            converted_value, marketplace, col, allowed_values
        )
        
        # –†–µ—à–µ–Ω–∏–µ –æ –∑–∞–ø–∏—Å–∏
        if final_value:
            value_to_set = final_value
        elif not allowed_values:
            value_to_set = converted_value
        else:
            logger.warning(f"‚ö†Ô∏è [{marketplace.upper()}] –ü—Ä–æ–ø—É—â–µ–Ω–æ '{converted_value}' –¥–ª—è '{col}' (–Ω–µ –ø—Ä–æ—à–ª–æ validation)")
            return 0
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ
        try:
            series = df[col]
            if isinstance(series, pd.DataFrame):
                series = series.iloc[:, 0]
            
            if pd.api.types.is_numeric_dtype(series.dtype):
                value_to_set = pd.to_numeric(value_to_set, errors='coerce')
            
            df.at[idx, col] = value_to_set
            self._log_change(marketplace, article, col, value_to_set)
            return 1
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏—è: {e}")
            return 0
    
    def _fill_composite_dimensions(
        self, dfs: Dict, article: str, col_yandex: str,
        source_unit, unit_wb, unit_ozon, yandex_data: Dict, current_value
    ) -> int:
        """–ó–∞–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –≥–∞–±–∞—Ä–∏—Ç—ã –≤ –Ø–Ω–¥–µ–∫—Å"""
        if isinstance(current_value, pd.Series):
            current_value = current_value.iloc[0] if not current_value.empty else None
        
        if pd.notna(current_value) and str(current_value).strip():
            return 0
        
        if article not in yandex_data:
            return 0
        
        idx = yandex_data[article]['index']
        composite = None
        
        # –ò–∑ WB
        if source_unit == unit_wb:
            wb_row = dfs['wildberries'][
                dfs['wildberries'][ARTICLE_COLUMNS['wildberries']].astype(str).str.strip() == article
            ]
            if not wb_row.empty:
                wb_row = wb_row.iloc[0]
                length = wb_row.get(DIMENSIONS_MAPPING['wildberries']['length'])
                width = wb_row.get(DIMENSIONS_MAPPING['wildberries']['width'])
                height = wb_row.get(DIMENSIONS_MAPPING['wildberries']['height'])
                
                if all(pd.notna(v) for v in [length, width, height]):
                    composite = DimensionsSynchronizer.format_composite_dimensions(
                        float(length), float(width), float(height)
                    )
        
        # –ò–∑ Ozon
        elif source_unit == unit_ozon:
            ozon_row = dfs['ozon'][
                dfs['ozon'][ARTICLE_COLUMNS['ozon']].astype(str).str.strip() == article
            ]
            if not ozon_row.empty:
                ozon_row = ozon_row.iloc[0]
                length_mm = ozon_row.get(DIMENSIONS_MAPPING['ozon']['length'])
                width_mm = ozon_row.get(DIMENSIONS_MAPPING['ozon']['width'])
                height_mm = ozon_row.get(DIMENSIONS_MAPPING['ozon']['height'])
                
                if all(pd.notna(v) for v in [length_mm, width_mm, height_mm]):
                    composite = DimensionsSynchronizer.format_composite_dimensions(
                        ValueConverter.mm_to_cm(float(length_mm)),
                        ValueConverter.mm_to_cm(float(width_mm)),
                        ValueConverter.mm_to_cm(float(height_mm))
                    )
        
        if composite:
            dfs['yandex'].at[idx, col_yandex] = composite
            self._log_change('yandex', article, col_yandex, composite)
            return 1
        
        return 0
    
    def _create_article_map(self, df: pd.DataFrame, article_col: str, value_col: str) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∞—Ä—Ç–∏–∫—É–ª -> {index, value}"""
        article_map = {}
        
        for idx, row in df.iterrows():
            article = row.get(article_col)
            if pd.notna(article) and str(article).strip():
                article_str = str(article).strip()
                article_map[article_str] = {
                    'index': idx,
                    'value': row.get(value_col)
                }
        
        return article_map
    
    def _postprocess_wb_dimensions(self, dfs: Dict[str, pd.DataFrame]) -> int:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–∞–±–∞—Ä–∏—Ç–æ–≤ WB: –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–º ‚Üí —Å–º –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ Ozon"""
        if 'wildberries' not in dfs:
            return 0
        
        converted_count = 0
        wb_map = DIMENSIONS_MAPPING['wildberries']
        df_wb = dfs['wildberries']
        
        for col in [wb_map['length'], wb_map['width'], wb_map['height']]:
            if col not in df_wb.columns:
                continue
            
            for idx, value in df_wb[col].items():
                if pd.notna(value):
                    try:
                        numeric_value = float(value)
                        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ > 100, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –º–∏–ª–ª–∏–º–µ—Ç—Ä—ã
                        if numeric_value > 100:
                            df_wb.at[idx, col] = ValueConverter.mm_to_cm(numeric_value)
                            converted_count += 1
                    except (ValueError, TypeError):
                        pass
        
        return converted_count
    
    def _save_results(self, synced_dfs: Dict[str, pd.DataFrame], output_paths: Dict[str, str]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        from utils.excel_writer import ExcelWriter
        writer = ExcelWriter()
        
        for marketplace, output_path in output_paths.items():
            if marketplace in synced_dfs:
                original_path = self.original_file_paths.get(marketplace)
                if original_path:
                    config = FILE_CONFIGS[marketplace]
                    writer.save_with_formatting(
                        synced_dfs[marketplace],
                        original_path,
                        output_path,
                        config['sheet_name'],
                        config['header_row']
                    )
                    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    
    def _log_change(self, marketplace: str, article: str, column: str, new_value, source_marketplace: str = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–µ"""
        self.changes_log[marketplace].append({
            'article': article,
            'column': column,
            'new_value': new_value,
            'source': source_marketplace
        })
    
    def _create_ai_log_sheet_in_report(self, report_path: str):
        """–°–æ–∑–¥–∞–µ—Ç –ª–∏—Å—Ç —Å AI-–ª–æ–≥–∞–º–∏ –≤ –æ—Ç—á–µ—Ç–µ"""
        if not self.ai_validation_log:
            return
        
        from openpyxl import load_workbook
        
        wb = load_workbook(report_path)
        ws = wb.create_sheet("AI Validation Log")
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        headers = ['–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å', '–°—Ç–æ–ª–±–µ—Ü', '–ò—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ', '–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ —Å', '–ú–µ—Ç–æ–¥']
        ws.append(headers)
        
        # –î–∞–Ω–Ω—ã–µ
        for log_entry in self.ai_validation_log:
            ws.append([
                log_entry['–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å'],
                log_entry['–°—Ç–æ–ª–±–µ—Ü'],
                log_entry['–ò—Å—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ'],
                log_entry['–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ —Å'],
                log_entry['–ú–µ—Ç–æ–¥']
            ])
        
        wb.save(report_path)
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ª–∏—Å—Ç 'AI Validation Log' –≤ {report_path}")
